<?php

return [
    'titre' => 'Help and understanding',
    'titre3' => 'A neural network for object detection',
    'detection' => 'Detection',
    'text_detection' => 'AI detection uses artificial intelligence algorithms to identify and analyze patterns in data. It is commonly used for image recognition, fraud detection, and text analysis. AI detection systems learn from large amounts of training data, allowing for continuous improvement in their accuracy. By leveraging techniques such as machine learning and deep learning, these systems can automate complex tasks. By integrating sensors and neural networks, AI detection can provide real-time responses and adapt to changing environments.',
    'segmentation' => 'Segmentation',
    'text_segmentation' => 'Segmentation in artificial intelligence involves dividing an image or dataset into smaller, homogeneous segments. In computer vision, this technique allows for the identification and isolation of specific objects or regions within an image. For example, image segmentation can separate different parts of a face or distinguish objects on a road for an autonomous vehicle. Segmentation methods use machine learning and deep learning algorithms to improve accuracy and efficiency. This technique is essential for applications such as medical analysis, robotics, and surveillance.',
    'pose' => 'Pose',
    'text_pose' => 'Pose estimation in artificial intelligence involves determining the position and orientation of objects or people in an image or video. This technique uses computer vision and deep learning algorithms to identify key points of the human body or features of an object. For instance, pose estimation can identify joints and movements of a person for augmented reality applications or sports tracking. By analyzing the relationships between these key points, the system can understand and mimic movements in a three-dimensional space. This technology is used in fields such as healthcare, security, and entertainment to enhance human-machine interaction.',
    'obb' => 'OBB',
    'text_obb' => 'Oriented Bounding Boxes (OBB) detection and orientation in artificial intelligence involves creating boxes that encapsulate objects with precise orientation. Unlike traditional bounding boxes, which are aligned with the image axes, OBBs can tilt to better fit the shape of objects. This technique is particularly useful in applications such as text recognition in complex environments, object detection in aerial or satellite images, and urban scene analysis. Deep learning algorithms are often used to learn to detect and draw these OBBs, allowing for more accurate detection adapted to the real shapes and orientations of objects.',
    'introduction' => 'We humans have a fast and accurate visual system, which allows us to perform complex tasks such as driving with little conscious thought. Indeed, we unconsciously know what objects we see, where they are, and how they interact.',
    'algorithms_overview' => 'In recent years, fast and accurate algorithms have been developed for object recognition in images. More details on these various algorithms can be found on the Wikipedia page: https://en.wikipedia.org/wiki/Object_recognition. Object recognition is a general term for a set of computer vision tasks that involve identifying objects in digital photographs. There are different types of object recognition:',
    'image_classification' => '- Image classification involves predicting the class of an object in an image.',
    'image_classification_input' => '- Input: an image with a single object.',
    'image_classification_output' => '- Output: a class label.',
    'object_localization' => '- Object localization involves identifying the location of one or more objects in an image and drawing a bounding box around their extent.',
    'object_localization_input' => '- Input: an image with one or more objects.',
    'object_localization_output' => '- Output: one or more bounding boxes.',
    'object_detection' => '- Object detection combines these two tasks and draws a bounding box around each object in the image and assigns them a class.',
    'object_detection_input' => '- Input: an image with one or more objects.',
    'object_detection_output' => '- Output: one or more bounding boxes and a class label for each bounding box.',
    'article_intro' => 'In this article, we will first overview the general approach of one of the neural networks that enables object detection in images, namely the You Only Look Once network commonly known as YOLO, then we will delve deeper into its behavior and architecture. Finally, we will see through a particular case the detection of objects on the road such as traffic lights, direction signs, pedestrians, etc., by installing a camera in a car and retrieving the images taken by it.',
    'general_approach' => 'General Approach',
    'yolo_family' => 'It is a popular family of object recognition models called YOLO or "You Only Look Once", first described by Joseph Redmon et al. in the 2015 paper "You Only Look Once: Unified, Real-Time Object Detection." (https://arxiv.org/abs/1506.02640). This model performs real-time object detection: it processes images at 30 frames per second (FPS).',
    'yolo_functioning' => 'How YOLO Works',
    'yolo_approach' => 'The approach involves a neural network that stitches together a multitude of neurons and takes a photo as input, directly predicting the bounding boxes and class labels for each bounding box.',
    'yolo_grid' => 'The model works by first dividing the input image into a grid of cells, where each cell is responsible for predicting a bounding box. A class prediction is also based on each cell.',
    'example_grid' => 'For example, an image can be divided into a 7 × 7 grid, and imagine each cell in the grid can predict 2 bounding boxes, resulting in 98 proposed bounding box predictions. The class probability map and the bounding boxes with confidence are then combined into a final set of bounding boxes and class labels. The image below summarizes the two outputs of the model.',
    'yolo_prediction_example' => 'Example of YOLO Prediction',
    'yolo_deeper_look' => 'Going Deeper',
    'yolo_grid_analysis' => 'Initially, as mentioned in the introduction, YOLO applies a grid to the image that does not serve to segment the image, to analyze each portion separately, as an R-CNN would. Indeed, R-CNN is an object detection architecture that starts by extracting regions of interest from the image, then uses these regions as input data for a CNN. This region separation allows for the detection of multiple objects of different classes in the same image. For more details on the workings of R-CNN, refer to the towards data science page: https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e.',
    'grid_application' => 'Grid Application',
    'yolo_anchor_boxes' => 'In contrast, with YOLO, the grid squares are used to generate a number of anchor boxes. These anchor boxes are one of the most difficult concepts to grasp when learning convolutional neural networks for object detection. It is also one of the most important parameters you can tune to improve the performance of your dataset. In fact, if the anchor boxes are not tuned correctly, your neural network will never know that certain small, large, or irregular objects exist and will never have the chance to detect them.',
    'anchor_box_definition' => 'An anchor box is a bounding box (a rectangle on the image) defined by a certain height and width (see the image below in blue). In fact, the use of anchor boxes allows a network to detect multiple objects, objects of different scales, and overlapping objects.',
    'anchor_box_generation' => 'Generating Anchor Boxes',
    'low_prob_anchor_boxes' => 'The first step, naturally, is to get rid of all the anchor boxes that have a low probability of detecting an object. This can be done by constructing a boolean mask (tf.boolean_mask in TensorFlow) and keeping only the boxes that have a probability above a certain threshold. This step eliminates abnormal object detections.',
    'anchor_box_filtering' => 'However, even after such filtering, we end up with many anchor boxes for each detected object. But we only need one box. This one is calculated using non-max suppression https://leonardoaraujosantos.gitbook.io/artificial-inteligence/machine_learning/deep_learning/single-shot-detectors/yolo#:~:text=Maxima%20Suppression%20(nms)-,During%20prediction,-time%20(after%20training',
    'nms_process' => 'Non-max suppression works as follows:',
    'nms_step1' => '1. Remove anchor boxes with a probability below a certain threshold, 0.6 for example;',
    'nms_step2' => '2. Select the anchor box with the highest detection probability;',
    'nms_step3' => '3. Remove anchor boxes that intersect the selected anchor box in 2) with an IoU greater than a certain threshold, usually 0.5. In other words, remove anchor boxes that are too close to each other at this step because they label the same object;',
    'nms_step4' => '4. Repeat steps 2 and 3 until there are no more anchor boxes to process.',
    'iou_concept' => 'In step 3, non-max suppression uses a concept called "Intersection over Union" or IoU. It takes as input two anchor boxes and, as the name suggests, calculates the ratio of their intersection and union.',
    'iou_example' => 'To better understand how this index is calculated, let\'s zoom in on the image and focus on the green and yellow anchor boxes:',
    'iou_calculation' => 'Calculating IoU',
    'iou_formula' => 'IoU is calculated by dividing the intersection of the 2 anchor boxes by their union. In other words, the formula is to divide the red part by the blue part.',
    'iou_interpretation' => 'In general, it is considered that:',
    'iou_threshold' => 'If IoU > 0.5 between 2 anchor boxes, it means they label the same object.',
    'final_image' => 'After running the algorithm, the final image is as follows. Here, we can see that IoU reduces the selection of anchor boxes from about twenty to just 2, which characterize 2 objects, namely the car and the person:',
    'yolo_final_detection' => 'Final Object Detection with YOLO',
    'yolo_practice_image' => 'In this example, it seems simple: the grid is small, there are few anchor boxes... But in practice, it looks more like the image below. In this sequence of images, we notice that the image has been divided into several squares (image 1) which generate several anchor boxes (image 2). The IoU is then calculated to group the anchor boxes that detect the same object (we can see this through the color groups on image 3). Finally, thanks to the Non-Max Suppression algorithm, we go from thousands of anchor boxes to only 2 to detect a dog and a bicycle (image 4):',
    'yolo_architecture' => 'YOLO Architecture',
    'yolo_architecture_details' => 'After studying the general behavior of the YOLO neural network, we will now focus on its architecture. YOLO consists of a total of 24 convolutional layers followed by 2 fully connected layers. The layers are separated by their functionality as follows:',
    'yolo_layers' => '- The first 20 convolutional layers are pre-trained on the ImageNet 1000-class classification dataset.',
    'yolo_layer_composition' => '- The layers include 1x1 reduction layers and 3x3 convolutional layers.',
    'additional_layers' => '- The last 4 convolutional layers followed by 2 fully connected layers are added to train the network for object detection with our database.',
    'final_layer' => '- The final layer predicts class probabilities and bounding boxes.',
    'transfer_learning' => '→ This is a classic transfer learning technique. A model already trained on a fairly generic task is taken and specialized for a task of interest by retraining only the last layers of the network with data specific to this task. (For more information: https://en.wikipedia.org/wiki/Transfer_learning)',
    'yolo_input_output' => '→ The input is a 448 x 448 image and the output is the class prediction of the object enclosed in the bounding box.',
    'yolo_architecture_image' => 'YOLO Architecture',
    'practice_autonomous_cars' => 'And in practice, what does it look like?',
    'case_autonomous_cars' => 'Specific Case: Autonomous Cars',
    'autonomous_car_intro' => 'It is now time to put into practice the concepts discussed in the previous sections through a specific case, namely the case of autonomous cars. Indeed, for a car to drive without a driver, several points must be implemented such as acceleration, braking, steering, etc. In our case, we will focus only on the detection of certain objects on the road, namely:',
    'camera_integration' => 'Our goal is to integrate a camera into a car so that this camera detects these objects on the road.',
    'database_creation' => 'Database Creation',
    'database_creation_details' => 'To achieve this goal, we start by installing a camera in a car moving in the city. We film the road from the passenger side of the car on a summer day (in this case study, different weather conditions were not taken into account) for 2 minutes and 30 seconds. Then, using a script, we capture one image for each second of video, for a total of 150 images. Below is an image from our database:',
    'database_image_example' => 'Example of an image from the database',
    'data_labeling' => 'Labeling',
    'labeling_process' => 'Once this database is created, we label these images using an open-source project named “YOLO Annotation Tool”, particularly suitable for use with YOLO data (https://github.com/ManzarIMalik/YOLO-Annotation-Tool).',
    'labeling_details' => 'This labeling involves, for each image in the dataset, drawing rectangles around the objects we want to recognize, specifying for each rectangle the class of the object it designates (red light, car, pedestrians...). As seen in the image below, we have drawn the 3 pink rectangles and assigned them the car class, and the 2 blue rectangles representing the red lights.',
    'labeling_example' => 'YOLO Annotation Tool: Example of Labeling',
    'labeling_output' => 'This will produce a text file for each image containing the class and position information of each labeled object on the image. Then, we use this text file to retrain the last 4 layers of the network for object detection with our database.',
    'algorithm_execution' => 'Algorithm Execution',
    'algorithm_training' => 'During the training of the last layers of the YOLO neural network with our database, here is what we see:',
    'training_output' => 'Outputs during YOLO training',
    'training_details' => '- 5981: number of iterations,',
    'avg_loss' => '- Avg loss: the average error → We expect it to be as small as possible, and we use this to decide when to stop the training;',
    'learning_rate' => '- Rate: learning rate used to update the weights;',
    'image_count' => '- Image: the total number of images for training;',
    'loading_time' => '- Loaded: the time to change images;',
    'avg_iou' => '- Region Avg IoU: Intersection over Union;',
    'class_prob' => '- Class: the average probabilities of true positives. For example, in the third line, 94.00% of the detected objects in this subdivision belong to their correct classes;',
    'obj_prob' => '- Obj: the probability that the box contains an object; In this example, the training is 57.96% confident that the bounding boxes contain an object;',
    'no_obj_prob' => '- No Obj: same as previously but for areas where there is no object, so we expect low values;',
    'avg_recall' => '- Avg Recall: among the objects to be detected, the number of objects it actually detected;',
    'object_count' => '- Count: the total number of objects detected in this subdivision.',
    'training_result' => 'Once the network is trained, here is the result on a given image:',
    'result_image' => 'Result after training YOLO on a given image',
    'result_observation' => 'We notice that the network has correctly detected the three direction signs and the two cars.',
    'conclusion' => 'Conclusion',
    'yolo_summary' => 'YOLO is a neural network that processes the entire image in one go, hence the name “You Only Look Once”, allowing it to perform real-time detection on videos. Recall that it processes images at 30 frames per second (FPS). This real-time detection is a real advantage, especially for specific use cases such as the autonomous car example or augmented reality.',
    'yolo_application' => 'Indeed, after installing a camera in a moving car and after retrieving the images taken by the camera, YOLO allowed us to perform object detection on the road such as traffic lights, direction signs, pedestrians, and cars.',
    'alternative_models' => 'However, YOLO is not the only existing model capable of detection. There are several others such as R-CNN, fast R-CNN, faster R-CNN...',
    'yolo_speed_advantage' => 'But what makes YOLO different from these is its execution speed. Indeed, models of the "R-CNN" type will define, for each input image (according to methods specific to the different versions of R-CNN), a set of regions likely to contain a searched object. The neural network will analyze each of these regions separately and determine whether they contain an object or not, and if so, the corresponding class. For each input image, the network will therefore have to define and analyze a multitude of regions, which takes time.',
];
